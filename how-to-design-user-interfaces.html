<!DOCTYPE html>
<html>
	<head>

		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
		
		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
		
		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
		
		<link rel="stylesheet" href="style.css" />
		
		<title>How to design user interfaces</title>
		
	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/ios-android.jpg" class="img-responsive" />		
		<small>Credit: BGR.com</small>
		
		<h1>How to design user interfaces</h1>
		<div class="lead">Andrew J. Ko</div>

		<p>For most of these readings, we've been talking about design in manner agnostic to the medium in which you're designing. You might use the methods and ideas we've talked about to design anything from toothbrushes to tablets. But media vary. Smartphones are different things than desktop computers. Tablets are different from augmented reality. Each of these media have their own challenges, their own constraints, and their own conventions. And so there's knowledge about each medium that a designer needs to know.</p>
		
		<p>In this class, we'll focus on <b>screen-based user interface design for digital computers</b>, since that's currently the dominant medium in society (this includes desktops, laptops, tablets, smartphones and even smart watches, but not augmented reality, virtual reality, or other non-screen interactions). Let's discuss some of the <a href="http://dx.doi.org/10.1145/344949.344959">core concepts in screen-based human-computer interaction</a> and some of the paradigms that exist in this medium.</p>
		
		<p>First, let's start with some basics you probably already know. Computers are functional machines, and so their behavior is governed by the concepts of <b>input</b>, <b>output</b>, and <b>algorithms</b>. Computers take input, process it with some algorithm, and then provide output. Everything about a human interface for a computer is therefore about these three things.</p>
		
		<p>Let's define each of them. <b>Input</b> is any kind of data that a user provides to a computer. Most often, these are keystrokes, mouse positions, mouse clicks, taps, and other sequences of low-level inputs. These lower-level inputs are usually aggregated into higher level inputs, such as clicks, taps, text strings, and gestures, which user interfaces then process. Every user interface accepts some <b>range</b> of input, and those inputs each have some <b>structure</b>. Part of a user interface is providing means by which users can express those inputs in valid structures.</p>
		
		<p><img class="img-responsive center-block" src="images/google.png" /></p>
		
		<p>What's the range of input that Google accepts on its search page? It has a text box, which accepts unicode text strings. It response to all kinds of keystrokes and accepts all kinds of symbols from all kinds of languages. The button on the right also accepts a mouse click. The low-level structure of the text input is a sequence of characters. Google takes that low level structure and parses in into a higher level structure of keywords, which it then uses as input to its search engage.</p>
		
		<p>The Google search page actually accepts many other <b>implicit inputs</b> too. There are a variety of personalization settings, such as search history, search preferences, and even sensor input (such as your location) that it also accepts as input. The user interface doesn't provide explicit controls for providing this input, but it is user input nonetheless.</p>
		
		</p>Some of these implicit inputs also have <b>default</b> values. For example, when you first used Google, your search history was empty, your language preference was chosen based on your IP address, and so on. Defaults represent a designers' beliefs of a user's most likely expectations, intents, and tasks.</p>

		<p><img class="img-responsive center-block" src="images/google-results.png" /></p>
		
		<p>The Google search results are the search engine's <b>output</b>. Like input, output can come in many forms and also has structure. The search results above include a stacked list of results, including a top result, several image search results, and a list of results. Each result has particular metadata that was computed and displayed.</p>
		
		<p>Inside the implementation of a user interface are several kinds of data and algorithms that determine it's behavior. One of the most central kinds of data is <b>state</b>. State is data stored in memory that represents information about an application it's current status. Think of it like a variable in a program that gets reflected somehow in the user interface's appearance or behavior. For example, consider an alarm clock. It's state includes things like the <em>current time</em>, an <em>alarm time</em> and a Boolean <em>alarm on</em> state. These states could be displayed to a user and modified by a user in a variety of ways. (Think of all of the alarm clock designs you've encountered in your life: they all have basically the same state with entirely different interfaces). All user interfaces respond to input by executing <b>event handlers</b>, which take the input and perform some operation on state, then provide some feedback to explain the result of the operation. Pressing a snooze button on an alarm, for example, sends input to an event handler, which responds to the input by disabling the alarm sound (setting the alarm on state to false) for a fixed period of time.</p>

		<p><img class="img-responsive center-block" style="width:50%" src="images/alarm.jpg" /></p>
		
		<p>A <b>mode</b> is a state that causes a user interface, given the same input, to provide different output depending on the value of the state.  For example, some alarm clocks have a switch that allows you to move between two "show time" and "change time" modes. This is captured in a two-valued "time mode" state, which is either "show time" or "change time." When the mode is ""show time" mode, the hour and minute buttons may do nothing, but when the mode is "change time," the buttons might increment the hour and minute. Because in our example a switch controls this mode, this is an example of a <b>passive mode</b>, which requires user input to set and unset (the user has to remember to pull the switch back to "show time" mode). There are also <b>active modes</b>. For example, imagine that instead of a switch to change modes, there was a button that had to be held down to change the time. People don't have to remember to exit this mode because by virtue of the physical action they performed to to enter the mode (pressing the key), the opposite physical action will exit the mode (releasing the key).</p>
		
		<p>Phew, that was a lot of terminology!</p>
		
		<p>Let's tie all of this terminology to design. <i>The primary goal of a user interface designer is to define inputs, outputs, and event handlers to modify state.</i> That means that before you ever make a user interface for something, you have to first decide what input, output, and state exist in your design, independent of how those are manifested in a user interface. This is a fundamentally larger question about the <b>information</b> and <b>behavior</b> that your application will have. You should really design these before you ever worry about the user interface for a design, as your user interface is completely dependent on your decisions about what an application stores and can do.</p>
		
		<p>Let's design a clock user interface for a smartphone. Let's start with a super simple clock that just displays the time. It has a "current time" state, a "setting time" mode. It accepts three types of inputs: a request to switch between showing the time and setting the time, and requests to increment the hour and minute. Because it accepts three inputs, it also has three event handlers to modify the "setting time" mode, and increment the hours and minutes. Note that we haven't yet said what any of this looks like or how it's laid out on the screen.</p>
		
		<p>If we adopt the aesthetic of designs being invisible, a good design would make the inputs, state, and outputs discoverable, clear, and efficient to use. That means we have to answer one big design question: how can we make it clear that to set the time, they need to 1) switch to time setting mode, 2) repeatedly increment the hours and minutes until reaching the current time, and 3) switch back to time display mode?</p>

		<p>To help us think about this we can use the concepts of <a href="http://dx.doi.org/10.1207/s15327051hci0104_2">gulfs of execution and evaluation</a>. The <b>gulf of execution</b> is the gap between what a person wants to do with an interface and what inputs are actually possible to provide. It's the struggle every person has to translate their goal into input that further their goal. For example, if you were using an alarm clock, one of the gulfs to bridge is how to make the alarm active; an interface with a big switch that says "on" and "off" has a small gulf for the user to bridge; they'll probably figure out what those buttons mean. An interface that has a similar switch hidden away in a menu that's not discoverable poses a much larger gulf of execution.</p>
		
		<p>The <b>gulf of evaluation</b> is the gap between the output and feedback an interface provides and a person's ability to relate that output to their goal. In our alarm example, if pressing the visible on/off to "on" made the switch visibly move to an "on" state (and perhaps even make a satisfying click sound), that's the interface bridging the gulf of evaluation, providing feedback to the user to help them understand the effect of pressing the switch. In the other interface where the switch was hidden, imagine a user pressing on the time, trying to see if that would activate the alarm, but the interface providing no feedback. That's a very large gulf of evaluation, requiring the user to guess what the lack of a response means.</p>

		<p>When you're designing a user interface, you're looking for a design that makes these gulfs as easy to bridge as possible. In most screen-based user interface design, bridging these gulfs requires a few strategies.</p>
		
		<p>The first strategy for bridging gulfs of execution is to make clear <a href="http://dx.doi.org/10.1080/01449290310001592587">affordances</a>. This means providing <b>perceptual cues</b> that tell a person that action is possible. For example, the little blinking vertical line in the text box (the text "caret") is affordance that says to the user "you can type stuff here". Lots of designs fail to provide affordances, but we learn them eventually anyway. For example, touch screens have <b>hidden affordances</b>, because there's very little about a perfectly flat glass surface that says "you can touch me to do things with data". Other devices have <b>false affordances</b>, such as websites that have text colored with the same color as links. These are perceptual cues that suggest the potential for action where there is none.</p>
		
		<p>To apply this strategy to our clock design, that means we want to provide some affordance for seeing that there's a time setting mode. Let's try this:</p>

		<p><img class="img-responsive center-block" style="width:25%" src="images/clock1.jpg" /></p>		
		
		<p>Does it work? It depends on whether the users would be familiar with this iOS <b>convention</b> of a switch looking like a little movable circle in a track. Conventions are design patterns (combinations of design decisions) that people have already learned. By using them, you don't have to teach something new to a person. That's why Apple requires all interfaces in its App Stores to use the same graphical user interface controls, the same typography, the same keyboard shortcuts, and similar layouts: this way, users can learn something once and reuse that knowledge everywhere.</p>
		
		<p>To account for users that might not know this convention, we could add some labels to help someone learn the convention:</p>
		
		<p><img class="img-responsive center-block" style="width:25%" src="images/clock2.jpg" /></p>		

		<p>Will users unfamiliar with the convention know that they can tap that switch toggle it? Maybe. It's worth usability testing. They'll probably try to tap the labels and nothing will happen and they'll get confused.</p>
		
		<p>To bridge the gulf of evaluation, you must provide <b>feedback</b> that is immediate and explains to the person viewing it what the computer did with their input. This is a teaching moment. Once someone enters are time setting mode, how will we teach them what to do next? We need to give them some feedback:</p>

		<p><img class="img-responsive center-block" style="width:25%" src="images/clock3.jpg" /></p>		
					
		<p>That's pretty direct and not very elegant, but I bet it's clear.</p>
		
		<p>How will we help them remember to switch back into show time mode? Ah, now we've come to a tricky design problem. We could keep adding more text to remind them to do it, but that gets pretty cluttered. We could also reconsider having a time setting mode altogether and instead just have users tap the hours and minutes whenever they want to change them. But that might be error prone, with people accidentally changing the time. The mode helps prevent those mistakes. Perhaps it could switch back automatically after some time? Or switch back when the user switches to another app? These are the joys of UI design: trying to find a way to balance simplicity, clarity, and convention.</p>

		<p>Now, there is a central aspect of UI that he have not discussed yet, and yet is likely one of the most important aspects of designing clear user interfaces: <b>typography</b>. Why did I choose to center the time above? And the controls? Why did I choose the font that I did? Let's see what happens if I change these choices in the worst possible way.</p>
		
		<p><img class="img-responsive center-block" style="width:25%" src="images/clock4.jpg" /></p>		

		<p>What makes this new design so much worse? Well, first, the left-to-right layout of the labels and the switch actually contained crucial information: when the switch is on the left, it's show time mode, and when it's on the right, it's set time mode. The new design is highly ambiguous. It's not even clear if the labels have anything to do with the switch because of the whitespace between the labels and the switch. And the prompt to set the time appears so far from the digits themselves, the word "tap" is no longer clear, because the proximity of the label clarified that "tap" meant "tap the digits just below this text". As you can see, where you place words, the fonts you use, the space between them, and the alignment between them all contain crucial information for understanding the meaning of text (and can easily convey <em>mis</em>information). If you ignore typography, you both ignore essential opportunities to bridge the gulf of execution and evaluation, but you risk widening those gulfs considerably.</p>

		<p>We will not go in depth into typography in this class, or into much more depth about user interface design. These basics, however, are the foundation you need to begin practicing.</p>					

		<center class="lead"><a href="how-to-be-critical.html">Next chapter: How to Critique</a></center>

		<h2>Further reading</h2>
		
		<p>Edwin L. Hutchins, James D. Hollan, and Donald A. Norman (1985). <a href="http://dx.doi.org/10.1207/s15327051hci0104_2">Direct Manipulation Interfaces</a>. Human-Computer Interaction, 1(4).</p>
		
		<p>Rex Hartson (2003). <a href="http://dx.doi.org/10.1080/01449290310001592587">Cognitive, physical, sensory, and functional affordances in interaction design</a>. Behaviour & Information Technology, 22(5).</p>

		<p>Myers, B., Hudson, S. E., & Pausch, R. (2000). <a href="http://dx.doi.org/10.1145/344949.344959">Past, present, and future of user interface software tools</a>. ACM Transactions on Computer-Human Interaction (TOCHI), 7(1), 3-28.</p>
		
	</body>

</html>


